{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r6bbiLmN_tVm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ],
      "metadata": {
        "id": "iIWLIGdx6d1Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TpTN9A9_tVs",
        "outputId": "db7dd884-6a09-4b08-82d1-97f6c4991635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOjfuuxi_tVu",
        "outputId": "013fc1ff-31d7-4a48-abcf-22709e3bfabb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9eDQTq7i_tV4"
      },
      "outputs": [],
      "source": [
        "vocab = set(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n_unoa9I_tWI"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vFwXG488_tWK"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0spqGueH_tWL",
        "outputId": "39606c6c-f39b-48da-83ee-255002fd81bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([45, 60, 55, ...,  4,  7,  9])>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b5__A7Bd_tWS"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DME1m149_tWW"
      },
      "outputs": [],
      "source": [
        "window_size = 100 # may be a hyperparam to optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNfYb_Iq_tWk",
        "outputId": "86113926-e49c-45d5-c834-1d01a27c68ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=TensorSpec(shape=(101,), dtype=tf.int64, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(window_size+1, drop_remainder=True)\n",
        "sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAWjrgrS_tWl",
        "outputId": "49d6df2f-1d24-465a-8a1a-95904498e17b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tes', 'est')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "def create_df(sequence):\n",
        "    input_seq = sequence[:-1]\n",
        "    label_seq = sequence[1:]\n",
        "    return input_seq, label_seq\n",
        "create_df(\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz_c-8hK_tWn",
        "outputId": "9747777a-8d76-4e39-b058-e111312ede8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "dataset = sequences.map(create_df)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB_D50zq_tWo",
        "outputId": "520c80d2-81c5-469c-c614-df83c7681e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : tf.Tensor(\n",
            "[45 60 55 13 33 21 12 60 33 60 61 30 56 24  9 35 30 57  6 55 30 21 65 30\n",
            " 21 51 55  6  3 30 30 23 21 37 56 46 21 57 62 55 33 42 30 55  2 21 42 30\n",
            " 37 55 21 20 30 21 13 51 30 37 38  7  9  9 22 47 47 24  9 29 51 30 37 38\n",
            "  2 21 13 51 30 37 38  7  9  9 45 60 55 13 33 21 12 60 33 60 61 30 56 24\n",
            "  9 58  6 62], shape=(100,), dtype=int64)\n",
            "Target: tf.Tensor(\n",
            "[60 55 13 33 21 12 60 33 60 61 30 56 24  9 35 30 57  6 55 30 21 65 30 21\n",
            " 51 55  6  3 30 30 23 21 37 56 46 21 57 62 55 33 42 30 55  2 21 42 30 37\n",
            " 55 21 20 30 21 13 51 30 37 38  7  9  9 22 47 47 24  9 29 51 30 37 38  2\n",
            " 21 13 51 30 37 38  7  9  9 45 60 55 13 33 21 12 60 33 60 61 30 56 24  9\n",
            " 58  6 62 21], shape=(100,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", input_example)\n",
        "    print(\"Target:\", target_example)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "G5kNe6KT_tWp"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000 # tf.data shuffles the data in a buffer instead of the memory so we have to give the buffer size (it does so cause\n",
        "# it could shuffle infinte size of data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tMPG32i_tWp",
        "outputId": "b468437c-6df1-4962-fdfb-453af9d7e8e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "dataset = (dataset\n",
        "        .shuffle(BUFFER_SIZE)\n",
        "        .batch(BATCH_SIZE, drop_remainder=True)\n",
        "        .prefetch(tf.data.experimental.AUTOTUNE)# overlaps the training and the preprocessing when executing step s the pipeline prepares data for step s+1\n",
        "          )\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "embedding = 256\n",
        "\n",
        "lstm_units = 1024"
      ],
      "metadata": {
        "id": "u_v7o_rPT4kk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "95Ke1Il0_tWq"
      },
      "outputs": [],
      "source": [
        "class mySuperModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, dim_embedding, lstm_units):\n",
        "        super().__init__(self)\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, dim_embedding)\n",
        "        self.lstm_1 = tf.keras.layers.LSTM(lstm_units,\n",
        "                                        return_sequences=True,\n",
        "                                        return_state=True)\n",
        "        self.lstm_2 = tf.keras.layers.LSTM(lstm_units,\n",
        "                                        return_sequences=True,\n",
        "                                        return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    \n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "        \n",
        "        if states is None:\n",
        "            state_1_h, state_1_c = self.lstm_1.get_initial_state(x)\n",
        "            states = [state_1_h, state_1_c]\n",
        "        x, state_1_h, state_1_c = self.lstm_1(x, initial_state=states, training=training)\n",
        "        states = [state_1_h, state_1_c]\n",
        "        x, state_2_h, state_2_c = self.lstm_2(x, initial_state=states, training=training)\n",
        "        states = [state_2_h, state_2_c]\n",
        "        x = self.dense(x, training=training)\n",
        "        \n",
        "        if  return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YHN7-gJD_tWr"
      },
      "outputs": [],
      "source": [
        "my_model = mySuperModel(vocab_size, embedding, lstm_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "z6iPE63K_tWs"
      },
      "outputs": [],
      "source": [
        "for input_seq, target in dataset.take(2):\n",
        "    prediction = my_model(input_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "ZzQDyTW5_tWs",
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RA5LuxpG_tWt",
        "outputId": "46a80b94-faa3-4755-9f47-8f0e0d89e366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_super_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " lstm (LSTM)                 multiple                  5246976   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,724,226\n",
            "Trainable params: 13,724,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "my_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "WAF0JlAP_tWv"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# why from_logits=True: https://datascience.stackexchange.com/\n",
        "#questions/73093/what-does-from-logits-true-do-in-sparsecategoricalcrossentropy-loss-function\n",
        "\n",
        "#https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVfHJ6eJ_tWw",
        "outputId": "99f854ee-2e08-4654-bca7-3865a402229c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189484, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target, prediction)\n",
        "print(\"Prediction shape: \", prediction.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5dYiM43S_tWx"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "trvedw4t_tWx"
      },
      "outputs": [],
      "source": [
        "my_model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3zul8J4u_tWy"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gceRqFSE_tWy"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJj1-9Zq_tWz",
        "outputId": "726a1fcd-c501-4734-a497-ad01c20a68df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 30s 171ms/step - loss: 2.3296\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 32s 185ms/step - loss: 1.8280\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 32s 185ms/step - loss: 1.5744\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 1.4362\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 1.3494\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 32s 184ms/step - loss: 1.2823\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 32s 186ms/step - loss: 1.2228\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 32s 185ms/step - loss: 1.1604\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 1.0951\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 32s 187ms/step - loss: 1.0271\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 32s 185ms/step - loss: 0.9529\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 0.8743\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 32s 186ms/step - loss: 0.7914\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 32s 186ms/step - loss: 0.7070\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 0.6234\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 33s 189ms/step - loss: 0.5416\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 32s 185ms/step - loss: 0.4623\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 32s 187ms/step - loss: 0.3906\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 32s 184ms/step - loss: 0.3277\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 32s 185ms/step - loss: 0.2704\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 32s 186ms/step - loss: 0.2208\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 0.1813\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 0.1488\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 32s 187ms/step - loss: 0.1244\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 32s 184ms/step - loss: 0.1080\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 32s 185ms/step - loss: 0.0962\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 32s 188ms/step - loss: 0.0872\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 0.0834\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 32s 186ms/step - loss: 0.0838\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 0.0893\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 32s 187ms/step - loss: 0.0997\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 32s 186ms/step - loss: 0.1073\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 0.1081\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 32s 185ms/step - loss: 0.0962\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 32s 186ms/step - loss: 0.0789\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 32s 185ms/step - loss: 0.0661\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 32s 185ms/step - loss: 0.0555\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 32s 188ms/step - loss: 0.0479\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 0.0442\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 32s 186ms/step - loss: 0.0462\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 0.0624\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 32s 187ms/step - loss: 0.1127\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 32s 186ms/step - loss: 0.1743\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 0.1638\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 32s 185ms/step - loss: 0.1103\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 32s 186ms/step - loss: 0.0710\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 32s 185ms/step - loss: 0.0445\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 32s 184ms/step - loss: 0.0277\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 32s 188ms/step - loss: 0.0167\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 32s 183ms/step - loss: 0.0093\n"
          ]
        }
      ],
      "source": [
        "hist_training = my_model.fit(dataset, epochs=EPOCHS)#, callbacks=[checkpoint_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_seq, target in dataset.take(1):\n",
        "    prediction = my_model(input_seq)"
      ],
      "metadata": {
        "id": "mjfYiXLDLHTj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "EaFVRkv_LOQW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_from_ids(my_model.predict(input_seq))"
      ],
      "metadata": {
        "id": "0ssFQDC4K6Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "k_sdwc3I_tW0"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_char):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_char\n",
        "        \n",
        "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            # Put a -inf at each bad index.\n",
        "            values=[-float('inf')]*len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            # Match the shape to the vocabulary\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "        \n",
        "    @tf.function\n",
        "    def generate_the_next_step(self, inputs, states=None, run_eagerly=True):\n",
        "        \n",
        "        inputs_ids = self.ids_from_chars(tf.strings.unicode_split(inputs, 'UTF-8')).to_tensor()\n",
        "        predicted_logits, states = self.model(inputs_ids, states, return_state=True)\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "        \n",
        "        #https://stackoverflow.com/questions/55063120/\n",
        "        #can-anyone-give-a-tiny-example-to-explain-the-params-of-tf-random-categorical\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "        \n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "        return predicted_chars, states\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "e2SiTKUs_tW0"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(my_model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Kqu5eJ3N_tW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cbe6b47-dc57-4355-ba2c-d246e2be72e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "if I had received them for the hire more; than broore, me to hororouse, than youredoure, than elous me thave eachie, he or thiar havith he bane bre, thilal hay the he oneste than bracar haince an ar have eacle an incle elace he on Clar he onoo hoone bracar ved ed yo be horedous:\n",
            "Bince an Ks houres yourge me me we me, thilar than e wit houre, an eare elovot me we me, thilar, she tha, bre and ve me, din the me me, me, not me thave more, me, te me we bele Cle ele an ince exe me thacle thored JUSESe an inglat he bre; me, noth thilan thial have me thably thile brorest me, he brouchie hoonour imoust tha thedowour'd the than JAndothat tha, he bres he onoo; me, de thavilachive, me we me bes me, thal he blachie he bro; swes me, house, thar inche blorere moouse, than Pre, me, car we more, cle an ince an ince wesoure, me me, dorery me thar the blachied he blores thiall Je havile de an ince ewele horestoure, thap thithoouth than O that me tar he on mo ve he onesto, than Cle an ince an ince ewe, than hoones thar younglial he than in \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 11.730036973953247\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['if I had received them for the hire'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_the_next_step(next_char, states=states)\n",
        "    #states = [state_h, state_c]\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "jFCbEoDY_tW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e412bffd-cc83-45c9-9929-15c6ce7a6175"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64,), dtype=string, numpy=\n",
              "array([b\"uld hide,\\nAs if I had received them for the hire\\nOf their breath only!\\n\\nMENENIUS:\\nDo not stand upon'\",\n",
              "       b'till cupboarding the viand, never bearing\\nLike labour with the rest, where the other instruments\\nDid',\n",
              "       b' Cominius\\nWith thee awhile: determine on some course,\\nMore than a wild exposture to each chance\\nThat',\n",
              "       b\"US:\\nI heard him swear,\\nWere he to stand for consul, never would he\\nAppear i' the market-place nor on\",\n",
              "       b' honour; and so, I pray, go with us.\\n\\nVIRGILIA:\\nGive me excuse, good madam; I will obey you in every',\n",
              "       b\" vouch, is more than that he hath,\\nBy many an ounce--he dropp'd it for his country;\\nAnd what is left\",\n",
              "       b' may be sworn by, both divine and human,\\nSeal what I end withal! This double worship,\\nWhere one part',\n",
              "       b\"More than thy fame and envy. Fix thy foot.\\n\\nMARCIUS:\\nLet the first budger die the other's slave,\\nAnd\",\n",
              "       b'll\\npatience; and, in roaring for a chamber-pot,\\ndismiss the controversy bleeding the more entangled\\n',\n",
              "       b\"o help Cominius.\\n\\nLARTIUS:\\nWorthy sir, thou bleed'st;\\nThy exercise hath been too violent for\\nA secon\",\n",
              "       b\" Rome\\nHer enemies' marks upon me. I do love\\nMy country's good with a respect more tender,\\nMore holy \",\n",
              "       b'se that you made your mother?\\n\\nCOMINIUS:\\nKnow, I pray you,--\\n\\nCORIOLANUS:\\nI know no further:\\nLet the',\n",
              "       b\"we do't,\\nThe dust on antique time would lie unswept,\\nAnd mountainous error be too highly heapt\\nFor t\",\n",
              "       b'US:\\nNow the fair goddess, Fortune,\\nFall deep in love with thee; and her great charms\\nMisguide thy op',\n",
              "       b'e rotten fens, whose loves I prize\\nAs the dead carcasses of unburied men\\nThat do corrupt my air, I b',\n",
              "       b\" it will in time\\nWin upon power and throw forth greater themes\\nFor insurrection's arguing.\\n\\nMENENIUS\",\n",
              "       b' had eleven die nobly for their\\ncountry than one voluptuously surfeit out of action.\\n\\nGentlewoman:\\nM',\n",
              "       b\", thus accused it:\\nThat only like a gulf it did remain\\nI' the midst o' the body, idle and unactive,\\n\",\n",
              "       b'ngle\\nhonour, in giving him our own voices with our own\\ntongues: therefore follow me, and I direct yo',\n",
              "       b\"hat's the matter?\\n\\nMessenger:\\nYou are sent for to the Capitol. 'Tis thought\\nThat Marcius shall be co\",\n",
              "       b\"econd Officer:\\nFaith, there had been many great men that have\\nflattered the people, who ne'er loved \",\n",
              "       b\" cannot,\\nBeing a Volsce, be that I am. Condition!\\nWhat good condition can a treaty find\\nI' the part \",\n",
              "       b'h too? what barm can your bisson\\nconspectuities glean out of this character, if I be\\nknown well enou',\n",
              "       b\"o better\\nThan have him hold that purpose and to put it\\nIn execution.\\n\\nBRUTUS:\\n'Tis most like he will\",\n",
              "       b\"how ye were disposed\\nEre they lack'd power to cross you.\\n\\nCORIOLANUS:\\nLet them hang.\\n\\nA Patrician:\\nA\",\n",
              "       b\"ength, and make us think\\nRather our state's defective for requital\\nThan we to stretch it out.\\nMaster\",\n",
              "       b'\\nTo the discontented members, the mutinous parts\\nThat envied his receipt; even so most fitly\\nAs you ',\n",
              "       b'many friends as enemies.\\n\\nMENENIUS:\\nSham it be put to that?\\n\\nFirst Senator:\\nThe gods forbid!\\nI prith',\n",
              "       b\"align our senators for that\\nThey are not such as you.\\n\\nFirst Citizen:\\nYour belly's answer? What!\\nThe\",\n",
              "       b\".\\n\\nFirst Citizen:\\nI twice five hundred and their friends to piece 'em.\\n\\nBRUTUS:\\nGet you hence instan\",\n",
              "       b\"cares fears; which will in time\\nBreak ope the locks o' the senate and bring in\\nThe crows to peck the\",\n",
              "       b\"as when I woo'd, in heart\\nAs merry as when our nuptial day was done,\\nAnd tapers burn'd to bedward!\\n\\n\",\n",
              "       b'odd; battles thrice six\\nI have seen and heard of; for your voices have\\nDone many things, some less, ',\n",
              "       b\"ut, we, have but pinn'd with rushes;\\nThey'll open of themselves.\\nHark you. far off!\\nThere is Aufidiu\",\n",
              "       b'he did before Corioli, call him,\\nWith all the applause and clamour of the host,\\nCAIUS MARCIUS CORIOL',\n",
              "       b\" of my standing here.\\n\\nThird Citizen:\\nWe do, sir; tell us what hath brought you to't.\\n\\nCORIOLANUS:\\nM\",\n",
              "       b\"is 'shall,'\\nHis popular 'shall' against a graver bench\\nThan ever frown in Greece. By Jove himself!\\nI\",\n",
              "       b'US:\\nMenenius ever, ever.\\n\\nHerald:\\nGive way there, and go on!\\n\\nCORIOLANUS:\\n\\nVOLUMNIA:\\nI have lived\\nTo',\n",
              "       b'yond clouds,\\nLet me deserve so ill as you, and make me\\nYour fellow tribune.\\n\\nSICINIUS:\\nYou show too ',\n",
              "       b' the wall, if\\nrenown made it not stir, was pleased to let him seek\\ndanger where he was like to find ',\n",
              "       b\"uth to o'er-peer. Rather than fool it so,\\nLet the high office and the honour go\\nTo one that would do\",\n",
              "       b'ove or hate\\nhim manifests the true knowledge he has in their\\ndisposition; and out of his noble carel',\n",
              "       b'his Antiates;\\nAnd that you not delay the present, but,\\nFilling the air with swords advanced and dart',\n",
              "       b\",\\nLadies and maids their scarfs and handkerchers,\\nUpon him as he pass'd: the nobles bended,\\nAs to Jo\",\n",
              "       b\"t's what I can; induced\\nAs you have been; that's for my country:\\nHe that has but effected his good w\",\n",
              "       b'urn to the tribunes.\\n\\nCORIOLANUS:\\nWell, what then? what then?\\n\\nMENENIUS:\\nRepent what you have spoke.',\n",
              "       b\"\\n'Twas from the canon.\\n\\nCORIOLANUS:\\n'Shall'!\\nO good but most unwise patricians! why,\\nYou grave but r\",\n",
              "       b\"lence, hail!\\nWouldst thou have laugh'd had I come coffin'd home,\\nThat weep'st to see me triumph? Ay,\",\n",
              "       b' Now, to seem to affect the malice and\\ndispleasure of the people is as bad as that which he\\ndislikes',\n",
              "       b' what you should, made you against the grain\\nTo voice him consul: lay the fault on us.\\n\\nBRUTUS:\\nAy, ',\n",
              "       b\"Nay, temperately; your promise.\\n\\nCORIOLANUS:\\nThe fires i' the lowest hell fold-in the people!\\nCall m\",\n",
              "       b'ust not. I wish\\nyou much mirth.\\n\\nVALERIA:\\nWell, then, farewell.\\n\\nMARCIUS:\\nYonder comes news. A wager',\n",
              "       b'kingly-crowned head, the vigilant eye,\\nThe counsellor heart, the arm our soldier,\\nOur steed the leg,',\n",
              "       b'ies. Have you an army ready, say you?\\n\\nVolsce:\\nA most royal one; the centurions and their charges,\\nd',\n",
              "       b'ardly in retire: believe me, sirs,\\nWe shall be charged again. Whiles we have struck,\\nBy interims and',\n",
              "       b\"swer\\nAs traitors do.\\n\\nCORIOLANUS:\\nThou wretch, despite o'erwhelm thee!\\nWhat should the people do wit\",\n",
              "       b\"marks of merit, wounds received for's country.\\n\\nSICINIUS:\\nWhy, so he did, I am sure.\\n\\nCitizens:\\nNo, \",\n",
              "       b\"have press'd a power, but it is not known\\nWhether for east or west: the dearth is great;\\nThe people \",\n",
              "       b'bsolute;\\nThough therein you can never be too noble,\\nBut when extremities speak. I have heard you say',\n",
              "       b\"! True, indeed! They ne'er cared for us\\nyet: suffer us to famish, and their store-houses\\ncrammed wit\",\n",
              "       b\"is this Antium. City,\\n'Tis I that made thy widows: many an heir\\nOf these fair edifices 'fore my wars\",\n",
              "       b' you.\\n\\nCORIOLANUS:\\nHow! no more!\\nAs for my country I have shed my blood,\\nNot fearing outward force, ',\n",
              "       b\" charge you, that you have contrived to take\\nFrom Rome all season'd office and to wind\\nYourself into\",\n",
              "       b'art,\\nThat is not glad to see thee! You are three\\nThat Rome should dote on: yet, by the faith of men,'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "text_from_ids(input_seq)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pPN9rNZdZOtm"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DofKEIAZo4J",
        "outputId": "dcf09cad-7d7c-4945-cde6-5fad0aaa0037"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kzNmSnhhaC2S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}